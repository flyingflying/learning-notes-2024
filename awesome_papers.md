
# Awesome Papers

## 经典 CNN 架构论文

+ [ ] LeNet: [Gradient-Based Learning Applied to Document Recognition](https://www.researchgate.net/publication/2985446)
+ [ ] AlexNet: [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
+ [ ] ZFNet: [Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)
+ [ ] VGG: [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556)
+ [ ] NiN: [Network In Network](https://arxiv.org/abs/1312.4400)
+ [ ] InceptionNet: [Going Deeper with Convolutions](https://arxiv.org/abs/1409.4842)
+ [ ] ResNet: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
+ [ ] EfficientNet: [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)
+ [ ] TextCNN: [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)

## 卷积层相关论文

+ [ ] Dilated Convolution: [Multi-Scale Context Aggregation by Dilated Convolutions](https://arxiv.org/abs/1511.07122)
+ [ ] [Deconvolutional Networks](https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf)
+ [ ] [Adaptive Deconvolutional Networks for Mid and High Level Feature Learning](https://www.matthewzeiler.com/mattzeiler/adaptivedeconvolutional.pdf)

## 模型参数初始化

+ [ ] Xavier Initialization: [Understanding the difficulty of training deep feedforward neural networks](https://proceedings.mlr.press/v9/glorot10a.html)
+ [ ] Kaiming Initialization: [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)

## GAN 相关论文

+ [ ] GAN: [Generative Adversarial Networks](https://arxiv.org/abs/1406.2661)
+ [ ] DCGAN: [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)

## GPT 系列论文

+ [ ] GPT-1: [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
+ [ ] GPT-2: [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
+ [ ] GPT-3: [Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
